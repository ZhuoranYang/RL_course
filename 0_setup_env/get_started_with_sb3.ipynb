{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/sb3/1_getting_started.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hyyN-2qyK_T2"
   },
   "source": [
    "# Stable Baselines3 Tutorial - Getting Started\n",
    "\n",
    "Github repo: https://github.com/araffin/rl-tutorial-jnrr19/tree/sb3/\n",
    "\n",
    "Stable-Baselines3: https://github.com/DLR-RM/stable-baselines3\n",
    "\n",
    "Documentation: https://stable-baselines3.readthedocs.io/en/master/\n",
    "\n",
    "SB3-Contrib: https://github.com/Stable-Baselines-Team/stable-baselines3-contrib\n",
    "\n",
    "RL Baselines3 zoo: https://github.com/DLR-RM/rl-baselines3-zoo\n",
    "\n",
    "[RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) is a training framework for Reinforcement Learning (RL), using Stable Baselines3.\n",
    "\n",
    "It provides scripts for training, evaluating agents, tuning hyperparameters, plotting results and recording videos.\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, you will learn the basics for using stable baselines library: how to create a RL model, train it and evaluate it. Because all algorithms share the same interface, we will see how simple it is to switch from one algorithm to another.\n",
    "\n",
    "\n",
    "## Install Dependencies and Stable Baselines3 Using Pip \n",
    "**We have already installed the packages**\n",
    "\n",
    "List of full dependencies can be found in the [README](https://github.com/DLR-RM/stable-baselines3).\n",
    "\n",
    "\n",
    "```\n",
    "pip install stable-baselines3[extra]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for autoformatting\n",
    "# %load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 784
    },
    "colab_type": "code",
    "id": "gWskDE2c9WoN",
    "outputId": "03477445-4249-49c3-ddba-4e12df09e98e"
   },
   "outputs": [],
   "source": [
    "# !apt-get install ffmpeg freeglut3-dev xvfb  # For visualization\n",
    "# !pip install \"stable-baselines3[extra]>=2.0.0a4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gymnasium[other]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FtY8FhliLsGm"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gcX8hEcaUpR0"
   },
   "source": [
    "Stable-Baselines3 works on environments that follow the [gym interface](https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html).\n",
    "You can find a list of available environment [here](https://gymnasium.farama.org/environments/classic_control/).\n",
    "\n",
    "Not all algorithms can work with all action spaces, you can find more in this [recap table](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BIedd7Pz9sOs"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ae32CtgzTG3R"
   },
   "source": [
    "The first thing you need to import is the RL model, check the documentation to know what you can use on which problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R7tKaBFrTR0a"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-0_8OQbOTTNT"
   },
   "source": [
    "The next thing you need to import is the policy class that will be used to create the networks (for the policy/value functions).\n",
    "This step is optional as you can directly use strings in the constructor: \n",
    "\n",
    "```PPO('MlpPolicy', env)``` instead of ```PPO(MlpPolicy, env)```\n",
    "\n",
    "Note that some algorithms like `SAC` have their own `MlpPolicy`, that's why using string for the policy is the recommended option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ROUJr675TT01"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.ppo.policies import MlpPolicy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RapkYvTXL7Cd"
   },
   "source": [
    "## Create the Gym env and instantiate the agent\n",
    "\n",
    "For this example, we will use CartPole environment, a classic control problem.\n",
    "\n",
    "\"A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. \"\n",
    "\n",
    "Cartpole environment: [https://gymnasium.farama.org/environments/classic_control/cart_pole/](https://gymnasium.farama.org/environments/classic_control/cart_pole/)\n",
    "\n",
    "![Cartpole](https://cdn-images-1.medium.com/max/1143/1*h4WTQNVIsvMXJTCpXm_TAw.gif)\n",
    "\n",
    "\n",
    "We chose the MlpPolicy because the observation of the CartPole task is a feature vector, not images.\n",
    "\n",
    "The type of action to use (discrete/continuous) will be automatically deduced from the environment action space\n",
    "\n",
    "Here we are using the [Proximal Policy Optimization](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html) algorithm, which is an Actor-Critic method: it uses a value function to improve the policy gradient descent (by reducing the variance).\n",
    "\n",
    "It combines ideas from [A2C](https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html) (having multiple workers and using an entropy bonus for exploration) and [TRPO](https://stable-baselines.readthedocs.io/en/master/modules/trpo.html) (it uses a trust region to improve stability and avoid catastrophic drops in performance).\n",
    "\n",
    "PPO is an on-policy algorithm, which means that the trajectories used to update the networks must be collected using the latest policy.\n",
    "It is usually less sample efficient than off-policy alorithms like [DQN](https://stable-baselines.readthedocs.io/en/master/modules/dqn.html), [SAC](https://stable-baselines3.readthedocs.io/en/master/modules/sac.html) or [TD3](https://stable-baselines3.readthedocs.io/en/master/modules/td3.html), but is much faster regarding wall-clock time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pUWGZp3i9wyf"
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch as th \n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Custom neural network of two layers of size 32 each with Relu activation function\n",
    "# Note: an extra linear layer will be added on top of the pi and the vf nets, respectively\n",
    "policy_kwargs = dict(activation_fn=th.nn.ReLU,\n",
    "                     net_arch=dict(pi=[32, 32], vf=[32, 32]))\n",
    "# Create the agent\n",
    "model = PPO(\"MlpPolicy\", \"CartPole-v1\", policy_kwargs=policy_kwargs, verbose=1, device =\"cpu\")\n",
    "\n",
    "# model = PPO(MlpPolicy, env, verbose=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4efFdrQ7MBvl"
   },
   "source": [
    "## We create a helper function to evaluate the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "63M8mSKR-6Zt"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.base_class import BaseAlgorithm\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    model: BaseAlgorithm,\n",
    "    num_episodes: int = 100,\n",
    "    deterministic: bool = True,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Evaluate an RL agent for `num_episodes`.\n",
    "\n",
    "    :param model: the RL Agent\n",
    "    :param env: the gym Environment\n",
    "    :param num_episodes: number of episodes to evaluate it\n",
    "    :param deterministic: Whether to use deterministic or stochastic actions\n",
    "    :return: Mean reward for the last `num_episodes`\n",
    "    \"\"\"\n",
    "    # This function will only work for a single environment\n",
    "    vec_env = model.get_env()\n",
    "    obs = vec_env.reset()\n",
    "    all_episode_rewards = []\n",
    "    for _ in range(num_episodes):\n",
    "        episode_rewards = []\n",
    "        done = False\n",
    "        # Note: SB3 VecEnv resets automatically:\n",
    "        # https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api\n",
    "        # obs = vec_env.reset()\n",
    "        while not done:\n",
    "            # _states are only useful when using LSTM policies\n",
    "            # `deterministic` is to use deterministic actions\n",
    "            action, _states = model.predict(obs, deterministic=deterministic)\n",
    "            # here, action, rewards and dones are arrays\n",
    "            # because we are using vectorized env\n",
    "            obs, reward, done, _info = vec_env.step(action)\n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "        all_episode_rewards.append(sum(episode_rewards))\n",
    "\n",
    "    mean_episode_reward = np.mean(all_episode_rewards)\n",
    "    print(f\"Mean reward: {mean_episode_reward:.2f} - Num episodes: {num_episodes}\")\n",
    "\n",
    "    return mean_episode_reward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zjEVOIY8NVeK"
   },
   "source": [
    "Let's evaluate the un-trained agent, this should be a random agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "xDHLMA6NFk95",
    "outputId": "231b2170-a607-48ed-e9d9-daef596f6384"
   },
   "outputs": [],
   "source": [
    "# Random Agent, before training\n",
    "mean_reward_before_train = evaluate(model, num_episodes=100, deterministic=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QjjPxrwkYJ2i"
   },
   "source": [
    "## Stable-Baselines already provides you with that helper:\n",
    "```\n",
    "evaluate_policy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8z6K9YImYJEx"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4oPTHjxyZSOL"
   },
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100, warn=False)\n",
    "\n",
    "print(f\"mean_reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r5UoXTZPNdFE"
   },
   "source": [
    "## Train the agent and evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e4cfSXIB-pTF"
   },
   "outputs": [],
   "source": [
    "# Train the agent for 10000 steps\n",
    "model.learn(total_timesteps=10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Training Output Breakdown**\n",
    "\n",
    "Here's a sample of training output \n",
    "\n",
    "```\n",
    "------------------------------------------\n",
    "| rollout/                |              |\n",
    "|    ep_len_mean          | 51.8         |\n",
    "|    ep_rew_mean          | 51.8         |\n",
    "| time/                   |              |\n",
    "|    fps                  | 1499         |\n",
    "|    iterations           | 5            |\n",
    "|    time_elapsed         | 6            |\n",
    "|    total_timesteps      | 10240        |\n",
    "| train/                  |              |\n",
    "|    approx_kl            | 0.0072779306 |\n",
    "|    clip_fraction        | 0.0377       |\n",
    "|    clip_range           | 0.2          |\n",
    "|    entropy_loss         | -0.639       |\n",
    "|    explained_variance   | 0.00668      |\n",
    "|    learning_rate        | 0.0003       |\n",
    "|    loss                 | 39.5         |\n",
    "|    n_updates            | 40           |\n",
    "|    policy_gradient_loss | -0.0129      |\n",
    "|    value_loss           | 70.8         |\n",
    "------------------------------------------ \n",
    "```\n",
    "\n",
    "This output is typically displayed by SB3 to provide real-time insights into the training progress.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Rollout Metrics (`rollout/`)**\n",
    "\n",
    "**Purpose:** Monitor the agent's performance during the rollout phase, where it interacts with the environment to collect experiences.\n",
    "\n",
    "- **`ep_len_mean` (Episode Length Mean):**  \n",
    "  - **Value:** `51.8`  \n",
    "  - **Explanation:** Represents the **average length** (in steps) of episodes over recent rollouts. A higher value indicates that the agent is sustaining interactions with the environment longer, which often correlates with better performance, especially in environments where the goal is to maximize episode duration (e.g., balancing tasks).\n",
    "\n",
    "- **`ep_rew_mean` (Episode Reward Mean):**  \n",
    "  - **Value:** `51.8`  \n",
    "  - **Explanation:** Denotes the **average cumulative reward** obtained per episode during recent rollouts. This metric is a direct indicator of the agent's performance—the higher the average reward, the better the agent is performing in achieving its objectives.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Time Metrics (`time/`)**\n",
    "\n",
    "**Purpose:** Provide information about the training process's temporal aspects, including speed and progression.\n",
    "\n",
    "- **`fps` (Frames Per Second):**  \n",
    "  - **Value:** `1499`  \n",
    "  - **Explanation:** Indicates the **number of frames (steps)** processed per second. A higher FPS means faster training, allowing the agent to learn more efficiently. This metric is influenced by factors like environment complexity, hardware capabilities, and whether multiple environments are running in parallel.\n",
    "\n",
    "- **`iterations`:**  \n",
    "  - **Value:** `5`  \n",
    "  - **Explanation:** Represents the **number of training iterations** completed. Each iteration typically involves collecting a batch of experiences and performing updates to the agent's policy and value networks.\n",
    "\n",
    "- **`time_elapsed`:**  \n",
    "  - **Value:** `6`  \n",
    "  - **Explanation:** Shows the **total time elapsed** since the start of training, usually in **seconds**. Given the high FPS and low iteration count, this suggests that the training is progressing rapidly.\n",
    "\n",
    "- **`total_timesteps`:**  \n",
    "  - **Value:** `10240`  \n",
    "  - **Explanation:** Denotes the **cumulative number of environment steps** taken by the agent across all parallel environments. This metric is crucial for understanding how much experience the agent has accumulated, which directly impacts learning.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Training Metrics (`train/`)**\n",
    "\n",
    "**Purpose:** Provide detailed insights into the optimization process, including losses, learning rates, and policy updates.\n",
    "\n",
    "- **`approx_kl` (Approximate Kullback-Leibler Divergence):**  \n",
    "  - **Value:** `0.0072779306`  \n",
    "  - **Explanation:** Measures the **difference between the new and old policy distributions**. PPO uses this metric to ensure that policy updates do not deviate excessively from the previous policy, maintaining stability. A smaller KL divergence indicates minor changes, whereas larger values suggest more significant updates.\n",
    "\n",
    "- **`clip_fraction`:**  \n",
    "  - **Value:** `0.0377`  \n",
    "  - **Explanation:** Indicates the **fraction of policy updates** that were **clipped** during the optimization step. PPO uses a clipping mechanism to restrict how much the policy can change in a single update, enhancing training stability. A low clip fraction means most updates were within the clipping range, whereas a higher value suggests more updates were subject to clipping.\n",
    "\n",
    "- **`clip_range`:**  \n",
    "  - **Value:** `0.2`  \n",
    "  - **Explanation:** The **clipping parameter** used in PPO. It defines the **maximum allowable change** in the policy ratio between the new and old policies. A standard value is `0.2`, meaning that the policy ratio is clipped to stay within `[1 - 0.2, 1 + 0.2]`.\n",
    "\n",
    "- **`entropy_loss`:**  \n",
    "  - **Value:** `-0.639`  \n",
    "  - **Explanation:** Represents the **entropy loss** component of the total loss. Entropy encourages exploration by penalizing certainty in action selection. A negative value indicates that the entropy is contributing positively to the loss, promoting more exploratory behavior.\n",
    "\n",
    "- **`explained_variance`:**  \n",
    "  - **Value:** `0.00668`  \n",
    "  - **Explanation:** Measures how well the value function predicts the returns. It ranges from `-∞` to `1`, where `1` means perfect prediction, `0` indicates no predictive power, and negative values suggest worse-than-mean predictions. A value close to `0` (as in this case) implies that the value function is not effectively capturing the variance in returns.\n",
    "\n",
    "- **`learning_rate`:**  \n",
    "  - **Value:** `0.0003`  \n",
    "  - **Explanation:** The **current learning rate** used by the optimizer. This parameter dictates the **step size** during gradient descent. A lower learning rate can lead to more stable but slower convergence, while a higher rate may speed up training but risk overshooting minima.\n",
    "\n",
    "- **`loss`:**  \n",
    "  - **Value:** `39.5`  \n",
    "  - **Explanation:** The **total loss** combining all components (policy loss, value loss, entropy loss). This scalar value guides the optimization process. Monitoring loss helps in diagnosing training progress and convergence behavior.\n",
    "\n",
    "- **`n_updates`:**  \n",
    "  - **Value:** `40`  \n",
    "  - **Explanation:** The **number of optimization steps** performed during the current iteration. PPO typically performs multiple updates per batch of collected experiences to refine the policy and value networks.\n",
    "\n",
    "- **`policy_gradient_loss`:**  \n",
    "  - **Value:** `-0.0129`  \n",
    "  - **Explanation:** The **policy loss** component derived from the policy gradient objective. A negative value indicates that the policy is improving (increasing the probability of better actions). This loss encourages the policy to favor actions that lead to higher rewards.\n",
    "\n",
    "- **`value_loss`:**  \n",
    "  - **Value:** `70.8`  \n",
    "  - **Explanation:** The **value function loss**, which measures the discrepancy between the predicted value and the actual returns. Minimizing this loss improves the accuracy of the value function, which is crucial for estimating future rewards.\n",
    "\n",
    "---\n",
    "\n",
    "### **Visualizing and Interpreting the Metrics**\n",
    "\n",
    "Understanding these metrics allows you to monitor the training process effectively:\n",
    "\n",
    "- **Performance Metrics (`rollout/`):**  \n",
    "  Track how well the agent is performing in the environment. An increase in `ep_rew_mean` and `ep_len_mean` typically indicates improving performance.\n",
    "\n",
    "- **Training Progress (`time/`):**  \n",
    "  Observe the speed (`fps`) and progression (`iterations`, `total_timesteps`) of training. High FPS is beneficial for faster experimentation.\n",
    "\n",
    "- **Optimization Health (`train/`):**  \n",
    "  - **Stability:** Low `approx_kl` and `clip_fraction` suggest stable policy updates.\n",
    "  - **Exploration vs. Exploitation:** Negative `entropy_loss` promotes exploration. Balancing this is key to effective learning.\n",
    "  - **Learning Efficiency:** Appropriate `learning_rate` and decreasing `value_loss` indicate efficient learning.\n",
    "  - **Value Function Accuracy:** Higher `explained_variance` signifies better value predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### **Practical Tips for Monitoring and Optimization**\n",
    "\n",
    "1. **Monitor Trends Over Time:**  \n",
    "   Rather than focusing on individual metric values, observe how they evolve over iterations. For instance, `ep_rew_mean` should generally increase, and `value_loss` should decrease as training progresses.\n",
    "\n",
    "2. **Adjust Hyperparameters Based on Metrics:**  \n",
    "   - If `clip_fraction` is too high, consider reducing `clip_range` to allow more significant policy updates.\n",
    "   - If `approx_kl` is consistently low, you might increase the `clip_range` to permit more substantial policy changes.\n",
    "   - A very low or negative `explained_variance` suggests that the value function needs improvement—consider modifying the network architecture or training parameters.\n",
    "\n",
    "3. **Ensure Balance Between Exploration and Exploitation:**  \n",
    "   Properly tuning `entropy_loss` is crucial. Too much exploration can prevent the agent from exploiting learned strategies, while too little can lead to premature convergence to suboptimal policies.\n",
    "\n",
    "4. **Optimize Learning Rate:**  \n",
    "   Monitor `learning_rate` and adjust if necessary. Learning rate schedulers can help in reducing the learning rate as training progresses, aiding in fine-tuning the policy.\n",
    "\n",
    "5. **Use Additional Logging and Visualization Tools:**  \n",
    "   Integrate tools like **Weights & Biases (wandb)** or **TensorBoard** to visualize these metrics over time, making it easier to diagnose and understand training dynamics.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ygl_gVmV_QP7"
   },
   "outputs": [],
   "source": [
    "# Evaluate the trained agent\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
    "\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A00W6yY3NkHG"
   },
   "source": [
    "Apparently the training went well, the mean reward increased a lot ! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xVm9QPNVwKXN"
   },
   "source": [
    "### Prepare video recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MPyfQxD5z26J"
   },
   "outputs": [],
   "source": [
    "# Set up fake display; otherwise rendering will fail\n",
    "import os\n",
    "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
    "os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SLzXxO8VMD6N"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "\n",
    "def show_videos(video_path=\"\", prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Taken from https://github.com/eleurent/highway-env\n",
    "\n",
    "    :param video_path: (str) Path to the folder containing videos\n",
    "    :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
    "    \"\"\"\n",
    "    html = []\n",
    "    for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
    "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
    "        html.append(\n",
    "            \"\"\"<video alt=\"{}\" autoplay \n",
    "                    loop controls style=\"height: 400px;\">\n",
    "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
    "                </video>\"\"\".format(\n",
    "                mp4, video_b64.decode(\"ascii\")\n",
    "            )\n",
    "        )\n",
    "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LTRNUfulOGaF"
   },
   "source": [
    "We will record a video using the [VecVideoRecorder](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecvideorecorder) wrapper, you will learn about those wrapper in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Trag9dQpOIhx"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
    "\n",
    "\n",
    "def record_video(env_id, model, video_length=500, prefix=\"\", video_folder=\"videos/\"):\n",
    "    \"\"\"\n",
    "    :param env_id: (str)\n",
    "    :param model: (RL model)\n",
    "    :param video_length: (int)\n",
    "    :param prefix: (str)\n",
    "    :param video_folder: (str)\n",
    "    \"\"\"\n",
    "    eval_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
    "    # Start the video at step=0 and record 500 steps\n",
    "    eval_env = VecVideoRecorder(\n",
    "        eval_env,\n",
    "        video_folder=video_folder,\n",
    "        record_video_trigger=lambda step: step == 0,\n",
    "        video_length=video_length,\n",
    "        name_prefix=prefix,\n",
    "    )\n",
    "\n",
    "    obs = eval_env.reset()\n",
    "    for _ in range(video_length):\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, _, _, _ = eval_env.step(action)\n",
    "\n",
    "    # Close the video recorder\n",
    "    eval_env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KOObbeu5MMlR"
   },
   "source": [
    "### Visualize trained agent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "iATu7AiyMQW2",
    "outputId": "68acb027-6c94-4389-8456-2cfb11494814"
   },
   "outputs": [],
   "source": [
    "record_video(\"CartPole-v1\", model, video_length=500, prefix=\"ppo-cartpole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-n4i-fW3NojZ"
   },
   "outputs": [],
   "source": [
    "show_videos(\"videos\", prefix=\"ppo-cartpole\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Y8zg4V566qD"
   },
   "source": [
    "## Bonus 1: Train a RL Model in One Line\n",
    "\n",
    "The policy class to use will be inferred and the environment will be automatically created. This works because both are [registered](https://stable-baselines3.readthedocs.io/en/master/guide/quickstart.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iaOPfOrwWEP4"
   },
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', \"CartPole-v1\", verbose=1).learn(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whole pipeline of training an RL agent using SB3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecVideoRecorder\n",
    "import torch as th\n",
    "import numpy as np\n",
    "import time \n",
    "\n",
    "# Environment ID\n",
    "env_id = \"Pendulum-v1\"\n",
    "\n",
    "# Create the Gymnasium environment\n",
    "env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "\n",
    "\n",
    "# Define custom policy network architecture\n",
    "policy_kwargs = dict(\n",
    "    activation_fn=th.nn.ReLU,\n",
    "    net_arch=dict(pi=[32, 32], vf=[32, 32])\n",
    ")\n",
    "\n",
    "# Initialize the PPO agent\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    verbose=0,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "# Evaluate the untrained agent\n",
    "print(\"=== Evaluating the untrained agent ===\")\n",
    "avg_reward_before, _ = evaluate_policy(model, env, n_eval_episodes=5, render=False)\n",
    "print(f\"Average Reward before training over 5 episodes: {avg_reward_before:.2f}\")\n",
    "\n",
    "record_video(\"Pendulum-v1\", model, video_length=100, prefix=\"ppo_untrained_pendulum\")\n",
    "\n",
    "start_time = time.time()\n",
    "# Train the PPO agent\n",
    "print(\"\\n=== Training the PPO agent ===\")\n",
    "model.learn(\n",
    "    total_timesteps=100_000,\n",
    "    log_interval=20   \n",
    ")\n",
    "end_time = time.time()\n",
    "training_time= end_time - start_time\n",
    "print(f\"\\n=== Training finished in {training_time:.2f} seconds ===\")\n",
    "\n",
    "# Evaluate the trained agent\n",
    "print(\"\\n=== Evaluating the trained agent ===\")\n",
    "avg_reward_after, _ = evaluate_policy(model, env, n_eval_episodes=5, render=False)\n",
    "print(f\"Average Reward after training over 5 episodes: {avg_reward_after:.2f}\")\n",
    "\n",
    "record_video(\"Pendulum-v1\", model, video_length=100, prefix=\"ppo_trained_pendulum\")\n",
    "\n",
    "# Structured Summary\n",
    "print(\"\\n=== Training Summary ===\")\n",
    "print(f\"Environment: {env_id}\")\n",
    "print(f\"Average Reward before Training: {avg_reward_before:.2f}\")\n",
    "print(f\"Average Reward after Training: {avg_reward_after:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_videos(\"videos\", prefix=\"ppo_untrained_pendulum\")\n",
    "show_videos(\"videos\", prefix=\"ppo_trained_pendulum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus 2: Multiple Environments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import time\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Parallel environments\n",
    "start_time = time.time()\n",
    "vec_env = make_vec_env(\"Pendulum-v1\", n_envs=8)\n",
    "print(\"\\n=== Training the PPO agent ===\")\n",
    "model = PPO(\"MlpPolicy\", vec_env, verbose=0)\n",
    "model.learn(total_timesteps=100000, log_interval=10)\n",
    "print(\"\\n=== Training finished ===\")\n",
    "end_time = time.time()\n",
    "training_time= end_time - start_time\n",
    "print(f\"\\n=== Training finished in {training_time:.2f} seconds ===\")\n",
    "\n",
    "\n",
    "avg_reward_after, _ = evaluate_policy(model, vec_env, n_eval_episodes=5, render=False)\n",
    "print(f\"Average Reward after training over 5 episodes: {avg_reward_after:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FrI6f5fWnzp-"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook we have seen:\n",
    "- how to define and train a RL model using stable baselines3, it takes only one line of code ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "73ji3gbNDkf7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "1_getting_started.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

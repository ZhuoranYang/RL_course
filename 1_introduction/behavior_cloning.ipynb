{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1283fae2-cc98-4346-a8ce-e2e17840d11d",
   "metadata": {},
   "source": [
    "## Imitation Learning (Behavior Cloning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d6378c72-57dc-4c52-b5fd-153a3f898e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "import random\n",
    "\n",
    "\n",
    "# Define the environment and expert policy\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "\n",
    "# Function to collect expert trajectories\n",
    "def collect_expert_data(env, policy, num_episodes=100):\n",
    "    states = []\n",
    "    for _ in range(num_episodes):\n",
    "        state, _ = env.reset()  # Gymnasium reset returns (state, info)\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy(state)\n",
    "            states.append(state)\n",
    "            state, _, done, _, _ = env.step(action)  # Gymnasium step returns (state, reward, done, truncated, info)\n",
    "    return np.array(states)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b23cc8-7300-4aa8-9254-47c2a46c28f6",
   "metadata": {},
   "source": [
    "## State visitation histogram under the expert policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e631ab8a-7944-414a-a4d2-c1e533f9e00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "expert_policy = lambda state: 2 if state[1] > 0 else 0  # Example expert policy\n",
    "\n",
    "# Collect 100 trajectories using the expert policy\n",
    "states = collect_expert_data(env, expert_policy)\n",
    "\n",
    "# Calculate the number of transition data points\n",
    "num_data_points = states.shape[0]\n",
    "\n",
    "# Plot the histogram of state values and density contours\n",
    "x, y = states[:, 0], states[:, 1]\n",
    "\n",
    "# Create a density plot\n",
    "kde = gaussian_kde([x, y])\n",
    "xx, yy = np.meshgrid(np.linspace(x.min(), x.max(), 100), np.linspace(y.min(), y.max(), 100))\n",
    "zz = kde(np.vstack([xx.ravel(), yy.ravel()])).reshape(xx.shape)\n",
    "\n",
    "# Plot the density contour\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.contourf(xx, yy, zz, levels=50, cmap=\"Blues\")\n",
    "plt.colorbar(label=\"Density\")\n",
    "plt.scatter(x, y, s=1, color=\"red\", label=\"State Samples\")\n",
    "plt.xlabel(\"Position\")\n",
    "plt.ylabel(\"Velocity\")\n",
    "plt.title(\"Density Contour of Expert Policy Trajectory\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"the number of data points is: {num_data_points}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582c5d34-5487-4e48-a26a-1679d0bfd008",
   "metadata": {},
   "source": [
    "## State visitation histogram under a noisy version of expert policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce11c5ae-c197-4586-ac0b-fe6e0ca3e25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the noisy expert policy\n",
    "epsilon = 0.2  # Probability of choosing a random action\n",
    "actions = [0, 1, 2]  # Possible actions: 0 (left), 1 (no push), 2 (right)\n",
    "\n",
    "noisy_expert_policy = lambda state: random.choice(actions) if random.random() < epsilon else (2 if state[1] > 0 else 0)\n",
    "\n",
    "# Collect trajectories using the noisy expert policy\n",
    "states_noisy = collect_expert_data(env, noisy_expert_policy)\n",
    "\n",
    "# Calculate the number of transition data points for the noisy policy\n",
    "num_data_points_noisy = states_noisy.shape[0]\n",
    "\n",
    "# Extract position and velocity\n",
    "x_noisy, y_noisy = states_noisy[:, 0], states_noisy[:, 1]\n",
    "\n",
    "# Create a density plot for the noisy policy\n",
    "kde_noisy = gaussian_kde([x_noisy, y_noisy])\n",
    "zz_noisy = kde_noisy(np.vstack([xx.ravel(), yy.ravel()])).reshape(xx.shape)\n",
    "\n",
    "# Plot the density contour for the noisy expert policy\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.contourf(xx, yy, zz_noisy, levels=50, cmap=\"Blues\")\n",
    "plt.colorbar(label=\"Density (Noisy Expert Policy)\")\n",
    "plt.scatter(x_noisy, y_noisy, s=1, color=\"red\", label=\"State Samples (Noisy)\")\n",
    "plt.xlabel(\"Position\")\n",
    "plt.ylabel(\"Velocity\")\n",
    "plt.title(\"Density Contour of Noisy Expert Policy Trajectory\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"the number of data points is: {num_data_points_noisy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e19f9b5-cbb3-4c6e-9263-2a8376e61f8d",
   "metadata": {},
   "source": [
    "# Now let's collect data and train a policy using behavior cloning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10864ba6-4d1b-4960-9b53-420f48cfdf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Verify GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Dataset class\n",
    "class ExpertDataset(Dataset):\n",
    "    def __init__(self, states, actions):\n",
    "        self.states = torch.tensor(states, dtype=torch.float32)\n",
    "        self.actions = torch.tensor(actions, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.states[idx], self.actions[idx]\n",
    "\n",
    "# Collect expert data\n",
    "def collect_expert_data_state_action(env, policy, num_episodes=10):\n",
    "    states, actions = [], []\n",
    "    max_steps = 1000  # Truncate episodes after 1000 steps\n",
    "    for _ in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        step = 0\n",
    "        while not done and step < max_steps:\n",
    "            action = policy(state)\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            state, _, done, _, _ = env.step(action)\n",
    "            step += 1\n",
    "    return np.array(states), np.array(actions)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20d4ce6-e746-4470-8cee-b81f91709b7c",
   "metadata": {},
   "source": [
    "### Collect a dataset and split into training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cd04e04a-9c08-4589-a5bb-be09324b5abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "states, actions = collect_expert_data_state_action(env, expert_policy, num_episodes=20)\n",
    "\n",
    "# Create a dataset and dataloaders\n",
    "dataset = ExpertDataset(states, actions)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed35606-0fc5-4e23-b013-6f7688cc424d",
   "metadata": {},
   "source": [
    "### Define policy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "80a520cd-9312-44b0-968e-fecedfd34515",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a policy network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "state_dim = states.shape[1]\n",
    "action_dim = 3  # For MountainCar-v0, actions are 0, 1, 2\n",
    "policy_net = PolicyNetwork(state_dim, action_dim).to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a53712-d6db-4989-9491-083a4f7e0c95",
   "metadata": {},
   "source": [
    "### Define loss function and optimization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e43e18b-f0a4-4fdb-9bd3-1531346d43c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 30\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    policy_net.train()\n",
    "    train_loss = 0\n",
    "    for states_batch, actions_batch in train_loader:\n",
    "        states_batch, actions_batch = states_batch.to(device), actions_batch.to(device)\n",
    "        assert torch.max(actions_batch) < action_dim, \"Target out of bounds for the output layer\"\n",
    "        assert torch.min(actions_batch) >= 0, \"Target contains negative values\"\n",
    "        optimizer.zero_grad()\n",
    "        outputs = policy_net(states_batch)\n",
    "        \n",
    "\n",
    "        loss = criterion(outputs, actions_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "\n",
    "    policy_net.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for states_batch, actions_batch in val_loader:\n",
    "            states_batch, actions_batch = states_batch.to(device), actions_batch.to(device)\n",
    "            outputs = policy_net(states_batch)\n",
    "            loss = criterion(outputs, actions_batch)\n",
    "            val_loss += loss.item()\n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "\n",
    "    # Print losses every few epochs\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:  # Print every 5 epochs and the first epoch\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}:\")\n",
    "        print(f\"  Training Loss: {train_losses[-1]:.4f}\")\n",
    "        print(f\"  Validation Loss: {val_losses[-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df1c517-ed84-4488-8e54-6918262d6d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot training and validation losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label=\"Training Loss\")\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss Curves\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddeee79-8148-4bbe-9bce-040ddb4ad1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    " \n",
    "\n",
    "# Define learned policy and expert policy functions\n",
    "learned_policy = lambda state: torch.argmax(policy_net(torch.tensor(state, dtype=torch.float32).to(device))).item()\n",
    "\n",
    "\n",
    "\n",
    "# Adjusted evaluation function\n",
    "def evaluate_policy(env, policy, num_episodes=20):\n",
    "    total_rewards = []\n",
    "    max_steps = 1000  # Add a step limit to prevent infinite loops\n",
    "    for _ in range(num_episodes):\n",
    "        state, _ = env.reset()  # Reset the environment\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        step = 0  # Step counter\n",
    "        while not done and step < max_steps:  # Add a step limit\n",
    "            action = policy(state)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            step += 1\n",
    "        total_rewards.append(total_reward)\n",
    "    return total_rewards\n",
    "\n",
    "# Evaluate the expert policy and learned policy\n",
    "expert_rewards = evaluate_policy(env, expert_policy, num_episodes=20)\n",
    "learned_rewards = evaluate_policy(env, learned_policy, num_episodes=20)\n",
    "\n",
    "# Plot histograms of rewards\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(expert_rewards, bins=10, alpha=0.7, label=\"Expert Policy\", edgecolor='black')\n",
    "plt.hist(learned_rewards, bins=10, alpha=0.7, label=\"Learned Policy\", edgecolor='black')\n",
    "plt.xlabel(\"Total Rewards\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of Rewards for Expert and Learned Policies\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Calculate and print the mean rewards\n",
    "mean_expert_rewards = np.mean(expert_rewards)\n",
    "mean_learned_rewards = np.mean(learned_rewards)\n",
    "\n",
    "print(f\"Mean Total Reward:\\n\"\n",
    "      f\"Expert Policy: {mean_expert_rewards:.2f}\\n\"\n",
    "      f\"Learned Policy: {mean_learned_rewards:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe36c8e2-b5cb-48f0-b119-687d4d879c65",
   "metadata": {},
   "source": [
    "## Let's run the same pipeline on noisy version of expert policy and see if it is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e5b21488-692f-4305-a496-49fc0a122acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "states, actions = collect_expert_data_state_action(env, noisy_expert_policy, num_episodes=50)\n",
    "\n",
    "# Create a dataset and dataloaders\n",
    "dataset = ExpertDataset(states, actions)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d413b467-f17b-47a5-b644-1fe8e1360336",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = states.shape[1]\n",
    "action_dim = 3  # For MountainCar-v0, actions are 0, 1, 2\n",
    "policy_net = PolicyNetwork(state_dim, action_dim).to(device)\n",
    "\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 30\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    policy_net.train()\n",
    "    train_loss = 0\n",
    "    for states_batch, actions_batch in train_loader:\n",
    "        states_batch, actions_batch = states_batch.to(device), actions_batch.to(device)\n",
    "        assert torch.max(actions_batch) < action_dim, \"Target out of bounds for the output layer\"\n",
    "        assert torch.min(actions_batch) >= 0, \"Target contains negative values\"\n",
    "        optimizer.zero_grad()\n",
    "        outputs = policy_net(states_batch)\n",
    "        \n",
    "\n",
    "        loss = criterion(outputs, actions_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "\n",
    "    policy_net.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for states_batch, actions_batch in val_loader:\n",
    "            states_batch, actions_batch = states_batch.to(device), actions_batch.to(device)\n",
    "            outputs = policy_net(states_batch)\n",
    "            loss = criterion(outputs, actions_batch)\n",
    "            val_loss += loss.item()\n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "\n",
    "    # Print losses every few epochs\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:  # Print every 5 epochs and the first epoch\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}:\")\n",
    "        print(f\"  Training Loss: {train_losses[-1]:.4f}\")\n",
    "        print(f\"  Validation Loss: {val_losses[-1]:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b071c49-a67c-446e-a371-d5274371e6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot training and validation losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label=\"Training Loss\")\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss Curves\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afd5f88-c288-46a4-a49d-6181dacb3dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define learned policy and expert policy functions\n",
    "learned_policy = lambda state: torch.argmax(policy_net(torch.tensor(state, dtype=torch.float32).to(device))).item()\n",
    "\n",
    "# Evaluate the noisy expert policy\n",
    "noisy_expert_rewards = evaluate_policy(env, noisy_expert_policy, num_episodes=20)\n",
    "\n",
    "# Plot histograms of rewards for expert, learned, and noisy expert policies\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(expert_rewards, bins=10, alpha=0.7, label=\"Expert Policy\", edgecolor='black')\n",
    "plt.hist(learned_rewards, bins=10, alpha=0.7, label=\"Learned Policy\", edgecolor='black')\n",
    "plt.hist(noisy_expert_rewards, bins=10, alpha=0.7, label=\"Noisy Expert Policy\", edgecolor='black')\n",
    "plt.xlabel(\"Total Rewards\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of Rewards for Expert, Learned, and Noisy Expert Policies\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Calculate and print the mean rewards\n",
    "mean_noisy_expert_rewards = np.mean(noisy_expert_rewards)\n",
    "\n",
    "print(f\"Mean Total Reward:\\n\"\n",
    "      f\"Expert Policy: {mean_expert_rewards:.2f}\\n\"\n",
    "      f\"Learned Policy: {mean_learned_rewards:.2f}\\n\"\n",
    "      f\"Noisy Expert Policy: {mean_noisy_expert_rewards:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0244601f-1c41-40b6-b197-fd65c348bb0c",
   "metadata": {},
   "source": [
    "## Let's try Dagger. \n",
    "\n",
    "In the previous experiment, we collected 20 episodes of expert data. Now let's collect only 10 episodes, and use the previously learned policy to collect another 10 episodes of data, and let the expert provide the label. Then we use this aggregated dataset (**still 20 episodes**) and train a policy from scratch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7ff0c927-d7e3-41cc-8d21-25a3aa8894df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Collect 10 episodes of expert data\n",
    "expert_states, expert_actions = collect_expert_data_state_action(env, expert_policy, num_episodes=10)\n",
    "\n",
    "# Step 2: Collect 10 episodes using the learned policy, with expert providing labels\n",
    "learned_states, _ = collect_expert_data_state_action(env, learned_policy, num_episodes=10)\n",
    "# Query the expert for the corresponding actions for the states collected by the learned policy\n",
    "learned_actions = np.array([expert_policy(state) for state in learned_states])\n",
    "\n",
    "# Step 3: Combine the two datasets\n",
    "dagger_states = np.vstack([expert_states, learned_states])\n",
    "dagger_actions = np.hstack([expert_actions, learned_actions])\n",
    "\n",
    "# Create a new dataset for fine-tuning\n",
    "dagger_dataset = ExpertDataset(dagger_states, dagger_actions)\n",
    "dagger_loader = DataLoader(dagger_dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88286186-ae2f-4b4b-ae1a-6828e9e0872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = states.shape[1]\n",
    "action_dim = 3  # For MountainCar-v0, actions are 0, 1, 2\n",
    "policy_net = PolicyNetwork(state_dim, action_dim).to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=1e-3)\n",
    "\n",
    "# Step 1: Finetune the policy network using the aggregated dataset (DAgger)\n",
    "dagger_epochs = 30\n",
    "dagger_losses = []\n",
    "\n",
    "for epoch in range(dagger_epochs):\n",
    "    policy_net.train()\n",
    "    dagger_loss = 0\n",
    "    for states_batch, actions_batch in dagger_loader:\n",
    "        states_batch, actions_batch = states_batch.to(device), actions_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = policy_net(states_batch)\n",
    "        loss = criterion(outputs, actions_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        dagger_loss += loss.item()\n",
    "    dagger_losses.append(dagger_loss / len(dagger_loader))\n",
    "\n",
    "# Step 2: Plot the DAgger training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, dagger_epochs + 1), dagger_losses, label=\"DAgger Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss with DAgger\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Step 3: Evaluate the DAgger-trained policy\n",
    "dagger_policy = lambda state: torch.argmax(policy_net(torch.tensor(state, dtype=torch.float32).to(device))).item()\n",
    "dagger_policy_rewards = evaluate_policy(env, dagger_policy, num_episodes=20)\n",
    "\n",
    "# Step 4: Plot histograms of rewards for Expert, Learned, and DAgger policies\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(expert_rewards, bins=10, alpha=0.7, label=\"Expert Policy\", edgecolor='black')\n",
    "plt.hist(learned_rewards, bins=10, alpha=0.7, label=\"Learned Policy\", edgecolor='black')\n",
    "plt.hist(dagger_policy_rewards, bins=10, alpha=0.7, label=\"DAgger Policy\", edgecolor='black')\n",
    "plt.xlabel(\"Total Rewards\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of Rewards for Expert, Learned, and DAgger Policies\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Step 5: Print the performance comparison\n",
    "mean_expert_rewards = np.mean(expert_rewards)\n",
    "mean_learned_rewards = np.mean(learned_rewards)\n",
    "mean_dagger_rewards = np.mean(dagger_policy_rewards)\n",
    "\n",
    "print(f\"Performance Comparison:\\n\"\n",
    "      f\"-----------------------\\n\"\n",
    "      f\"Expert Policy Performance: {mean_expert_rewards:.2f}\\n\"\n",
    "      f\"Initial Learned Policy Performance: {mean_learned_rewards:.2f}\\n\"\n",
    "      f\"DAgger Policy Performance: {mean_dagger_rewards:.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384678c0-c54b-401d-bbe3-bf705183b6b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec2f947-2c09-4e7e-9320-ebd8d49ebbcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

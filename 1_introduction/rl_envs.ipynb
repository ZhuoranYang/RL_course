{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore different Gymnasium environments\n",
    "\n",
    "In the following, we explore different Gymnasium environments (classic control, Atari, and MuJoCo) using random actions and a pre-trained policy. \n",
    "\n",
    "**Make sure you load ``rl_vis`` environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Environment Setup\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- We import Gymnasium (successor to OpenAI Gym), matplotlib for plotting and animations, and stable_baselines3 for loading a pretrained policy.\n",
    "- We define lists of environment IDs for classic control tasks, Atari games, and MuJoCo tasks.\n",
    "- The all_env_ids list will let us loop through environments easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Definitions\n",
    "\n",
    "# Set rendering backend for Mujoco \n",
    "import os\n",
    "os.environ['MUJOCO_GL'] = 'egl'\n",
    "os.environ['PYOPENGL_PLATFORM'] = 'egl'\n",
    "\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import imageio\n",
    "import os\n",
    "import base64\n",
    "from IPython.display import Image, display, clear_output\n",
    "from io import BytesIO\n",
    "from typing import Optional, Dict, Any, List\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define environment categories\n",
    "control_env_ids = [\n",
    "    \"CartPole-v1\",        # Balance a pole on a cart\n",
    "    \"MountainCar-v0\",     # Drive up a mountain\n",
    "    \"Acrobot-v1\",         # Swing up a double pendulum\n",
    "    \"Pendulum-v1\"         # Swing up a pendulum\n",
    "]\n",
    "\n",
    "box2d_env_ids = [\n",
    "    \"LunarLander-v2\",     # Land a spacecraft\n",
    "    \"BipedalWalker-v3\"    # Make a 2D robot walk\n",
    "]\n",
    "\n",
    "mujoco_env_ids = [\n",
    "    \"Hopper-v3\",          # Make a 2D one-legged robot hop\n",
    "    \"Humanoid-v3\"         # Make a 3D humanoid walk\n",
    "]\n",
    "\n",
    "\n",
    "atari_env_ids = [\n",
    "    \"PongNoFrameskip-v4\",        # Atari Pong\n",
    "    \"BreakoutNoFrameskip-v4\"     # Atari Breakout\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### `print_env_info` Function:\n",
    "\n",
    "```python\n",
    "def print_env_info(env_id: str) -> Dict[str, Any]:\n",
    "```\n",
    "\n",
    "This function prints and returns key environment information:\n",
    "\n",
    "1. **Observation Space**:\n",
    "   - Type (usually Box or Discrete)\n",
    "   - Shape (dimensions of state space)\n",
    "   - Bounds (min/max values possible)\n",
    "\n",
    "2. **Action Space**:\n",
    "   - Type (Box for continuous, Discrete for discrete actions)\n",
    "   - For discrete: number of possible actions\n",
    "   - For continuous: action dimensions and bounds\n",
    "\n",
    "3. **Time Horizon**:\n",
    "   - Maximum episode length\n",
    "   - Some environments have infinite horizons\n",
    "\n",
    "4. **Reward Range**:\n",
    "   - Minimum and maximum possible rewards\n",
    "   - Important for reward scaling in RL algorithms\n",
    "\n",
    "The function uses `hasattr()` to safely check for properties as different environment types may have different attributes.\n",
    "\n",
    "Would you like me to elaborate on any specific aspect of these environments or the code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_env_info(env_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Print detailed information about a Gymnasium environment\n",
    "    \"\"\"\n",
    "    env = gym.make(env_id)\n",
    "    info = {}\n",
    "    \n",
    "    print(f\"\\n{'='*20} Environment Info: {env_id} {'='*20}\")\n",
    "    \n",
    "    # State Space\n",
    "    print(\"\\nObservation Space:\")\n",
    "    print(f\"Type: {type(env.observation_space)}\")\n",
    "    print(f\"Shape: {env.observation_space.shape}\")\n",
    "    if hasattr(env.observation_space, 'low'):\n",
    "        print(f\"Bounds: [{env.observation_space.low.min()}, {env.observation_space.high.max()}]\")\n",
    "    info['observation_space'] = env.observation_space\n",
    "    \n",
    "    # Action Space\n",
    "    print(\"\\nAction Space:\")\n",
    "    print(f\"Type: {type(env.action_space)}\")\n",
    "    if hasattr(env.action_space, 'n'):\n",
    "        print(f\"Number of actions: {env.action_space.n}\")\n",
    "    elif hasattr(env.action_space, 'shape'):\n",
    "        print(f\"Action shape: {env.action_space.shape}\")\n",
    "        print(f\"Bounds: [{env.action_space.low.min()}, {env.action_space.high.max()}]\")\n",
    "    info['action_space'] = env.action_space\n",
    "    \n",
    "    # Time limit\n",
    "    if hasattr(env, '_max_episode_steps'):\n",
    "        print(f\"\\nTime Horizon: {env._max_episode_steps} steps\")\n",
    "        info['horizon'] = env._max_episode_steps\n",
    "    else:\n",
    "        print(\"\\nTime Horizon: Infinite or Unknown\")\n",
    "        info['horizon'] = None\n",
    "    \n",
    "    # Reward range\n",
    "    if hasattr(env, 'reward_range'):\n",
    "        print(f\"\\nReward Range: {env.reward_range}\")\n",
    "        info['reward_range'] = env.reward_range\n",
    "    \n",
    "    env.close()\n",
    "    return info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### `collect_trajectory_random_policy` Function:\n",
    "\n",
    "```python\n",
    "def collect_trajectory_random_policy(env_id: str, max_steps: Optional[int] = None) -> tuple:\n",
    "```\n",
    "\n",
    "This function:\n",
    "1. Creates environment with RGB array rendering mode\n",
    "2. Initializes lists for frames and rewards\n",
    "3. Sets episode length limit (uses environment's limit or default 1000)\n",
    "4. Runs episodes using random actions:\n",
    "   - Samples random action from action space\n",
    "   - Takes step in environment\n",
    "   - Collects reward and rendered frame\n",
    "   - Stops if environment terminates or truncates\n",
    "5. Returns frames and episode statistics (length, total/average reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_trajectory(env_id: str, policy=None, max_steps: Optional[int] = None) -> tuple:\n",
    "   \"\"\"\n",
    "   Collect trajectory using either random or trained policy and return frames and stats\n",
    "   \n",
    "   Parameters:\n",
    "   -----------\n",
    "   env_id : str\n",
    "       Gymnasium environment ID\n",
    "   policy : stable_baselines3 model or None\n",
    "       If None, uses random policy. Otherwise uses the trained policy\n",
    "   max_steps : int or None\n",
    "       Maximum steps per episode. If None, uses environment default\n",
    "   \n",
    "   Returns:\n",
    "   --------\n",
    "   frames : list\n",
    "       List of rendered frames as RGB arrays\n",
    "   episode_stats : dict\n",
    "       Statistics about the episode including length and rewards\n",
    "   \"\"\"\n",
    "   env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "   frames = []\n",
    "   rewards = []\n",
    "   \n",
    "   if max_steps is None:\n",
    "       max_steps = env._max_episode_steps if hasattr(env, '_max_episode_steps') else 1000\n",
    "   \n",
    "   obs, _ = env.reset()\n",
    "   \n",
    "   for step in range(max_steps):\n",
    "       # Get action from policy or random sampling\n",
    "       if policy is None:\n",
    "           action = env.action_space.sample()\n",
    "       else:\n",
    "           action, _ = policy.predict(obs, deterministic=True)\n",
    "           \n",
    "       obs, reward, terminated, truncated, info = env.step(action)\n",
    "       rewards.append(reward)\n",
    "       \n",
    "       frame = env.render()\n",
    "       frames.append(frame)\n",
    "       \n",
    "       if terminated or truncated:\n",
    "           break\n",
    "   \n",
    "   env.close()\n",
    "   \n",
    "   episode_stats = {\n",
    "       'length': len(frames),\n",
    "       'total_reward': sum(rewards),\n",
    "       'avg_reward': np.mean(rewards)\n",
    "   }\n",
    "   \n",
    "   return frames, episode_stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covert frames as Gif files and write a function to reprot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gif(frames, fps=30):\n",
    "    \"\"\"\n",
    "    Generates an animated GIF from a list of frames with infinite looping.\n",
    "\n",
    "    Args:\n",
    "        frames (list): List of frames (as NumPy arrays).\n",
    "        fps (int): Frames per second for the GIF.\n",
    "\n",
    "    Returns:\n",
    "        gif_bytes (bytes): The GIF image in bytes.\n",
    "    \"\"\"\n",
    "    with BytesIO() as buffer:\n",
    "        # 'loop=0' ensures the GIF loops infinitely\n",
    "        imageio.mimsave(buffer, frames, format='GIF', fps=fps, loop=0)\n",
    "        gif_bytes = buffer.getvalue()\n",
    "    return gif_bytes\n",
    "\n",
    "def display_frames_as_gif(frames, fps=30):\n",
    "    \"\"\"\n",
    "    Displays an animated GIF in the Jupyter notebook with infinite looping.\n",
    "\n",
    "    Args:\n",
    "        gif_bytes (bytes): The GIF image in bytes.\n",
    "    \"\"\"\n",
    "    gif_bytes = generate_gif(frames, fps)\n",
    "    display(Image(data=gif_bytes))\n",
    "\n",
    "    \n",
    "\n",
    "def analyze_environment_category(env_ids: List[str], category_name: str):\n",
    "    \"\"\"\n",
    "    Analyze all environments in a category and display their trajectories\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*30} {category_name} Environments {'='*30}\")\n",
    "    \n",
    "    for env_id in env_ids:\n",
    "        try:\n",
    "            print(f\"\\nAnalyzing {env_id}...\")\n",
    "            \n",
    "            # Print environment information\n",
    "            info = print_env_info(env_id)\n",
    "            \n",
    "            # Collect trajectory\n",
    "            frames, stats = collect_trajectory(env_id)\n",
    "            \n",
    "            print(\"\\nEpisode Statistics:\")\n",
    "            print(f\"Length: {stats['length']}\")\n",
    "            print(f\"Total Reward: {stats['total_reward']:.2f}\")\n",
    "            print(f\"Average Reward: {stats['avg_reward']:.2f}\")\n",
    "            \n",
    "            print(\"\\nTrajectory visualization:\")\n",
    "            display_frames_as_gif(frames)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with environment {env_id}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "## Classical Control Environments in Gymnasium\n",
    "\n",
    "Classical control environments implement fundamental problems from control theory and reinforcement learning. These environments are:\n",
    "\n",
    "1. **CartPole-v1**:\n",
    "   - A pole is attached to a cart moving along a frictionless track\n",
    "   - Goal: Apply forces to the cart to keep the pole upright\n",
    "   - State space: [cart position, cart velocity, pole angle, pole angular velocity]\n",
    "   - Action space: Discrete(2) - push left (0) or right (1)\n",
    "   - Terminal conditions: Pole angle > 15°, cart position > 2.4, or episode length > 500\n",
    "\n",
    "2. **MountainCar-v0**:\n",
    "   - A car must drive up a mountain\n",
    "   - Goal: Build enough momentum by driving back and forth to reach the top\n",
    "   - State space: [position, velocity]\n",
    "   - Action space: Discrete(3) - push left (0), no push (1), push right (2)\n",
    "   - Terminal conditions: Reaching the goal or episode length > 200\n",
    "\n",
    "3. **Acrobot-v1**:\n",
    "   - A two-link pendulum with one actuator\n",
    "   - Goal: Swing the end of the lower link above a certain height\n",
    "   - State space: [cos(θ₁), sin(θ₁), cos(θ₂), sin(θ₂), θ₁_dot, θ₂_dot]\n",
    "   - Action space: Discrete(3) - apply torque to joint\n",
    "   - Terminal conditions: End reaches target height or episode length > 500\n",
    "\n",
    "4. **Pendulum-v1**:\n",
    "   - An inverted pendulum swingup problem\n",
    "   - Goal: Apply torques to swing the pendulum upright\n",
    "   - State space: [cos(θ), sin(θ), angular velocity]\n",
    "   - Action space: Box(1) - continuous torque value\n",
    "   - Time limit: 200 steps\n",
    "\n",
    "### The following code prints the basic information of Classical Control Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env_ids, category_name = control_env_ids, \"Classical_Control\"\n",
    "\n",
    "analyze_environment_category(env_ids, category_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following code load trained policies from HuggingFace, which are trained by StableBaselines3 \n",
    "\n",
    "**Models**: https://huggingface.co/sb3 \n",
    "\n",
    "**Reference**: https://huggingface.co/docs/hub/stable-baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from huggingface_sb3 import load_from_hub\n",
    "from stable_baselines3 import PPO, DDPG, DQN, SAC\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.base_class import BaseAlgorithm\n",
    "import numpy as np\n",
    "from typing import Optional\n",
    "\n",
    "class GymToGymnasiumWrapper:\n",
    "    \"\"\"\n",
    "    Wrapper to make SB3 models work with Gymnasium environments\n",
    "    \"\"\"\n",
    "    def __init__(self, model: BaseAlgorithm):\n",
    "        self.model = model\n",
    "    \n",
    "    def predict(self, obs, deterministic=True):\n",
    "        action, _state = self.model.predict(obs, deterministic=deterministic)\n",
    "        return action, None\n",
    "\n",
    "def load_and_wrap_model(config: dict) -> GymToGymnasiumWrapper:\n",
    "    \"\"\"\n",
    "    Load model from HuggingFace and wrap it for Gymnasium compatibility\n",
    "    \"\"\"\n",
    "    checkpoint = load_from_hub(\n",
    "        repo_id=config[\"repo_id\"],\n",
    "        filename=config[\"filename\"]\n",
    "    )\n",
    "    model = config[\"algorithm\"].load(checkpoint)\n",
    "    return GymToGymnasiumWrapper(model)\n",
    "\n",
    "def collect_trajectory(env_id: str, policy=None, max_steps: Optional[int] = None) -> tuple:\n",
    "    \"\"\"\n",
    "    Collect trajectory using either random or trained policy\n",
    "    \"\"\"\n",
    "    env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "    frames = []\n",
    "    rewards = []\n",
    "    \n",
    "    if max_steps is None:\n",
    "        max_steps = env._max_episode_steps if hasattr(env, '_max_episode_steps') else 1000\n",
    "    \n",
    "    obs, _ = env.reset()\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        if policy is None:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action, _ = policy.predict(obs, deterministic=True)\n",
    "            \n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        frame = env.render()\n",
    "        frames.append(frame)\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    episode_stats = {\n",
    "        'length': len(frames),\n",
    "        'total_reward': sum(rewards),\n",
    "        'avg_reward': np.mean(rewards)\n",
    "    }\n",
    "    \n",
    "    return frames, episode_stats\n",
    "\n",
    "def compare_policies(env_ids: list):\n",
    "    \"\"\"\n",
    "    Compare random and trained policies with visualizations\n",
    "    \"\"\"\n",
    "    for env_id in env_ids:\n",
    "        print(f\"\\n{'='*20} Evaluating {env_id} {'='*20}\")\n",
    "        \n",
    "        # Random policy trajectory\n",
    "        print(\"\\nRandom Policy Trajectory:\")\n",
    "        frames_random, stats_random = collect_trajectory(env_id, policy=None)\n",
    "        print(\"Random Policy Stats:\")\n",
    "        print(f\"Episode Length: {stats_random['length']}\")\n",
    "        print(f\"Total Reward: {stats_random['total_reward']:.2f}\")\n",
    "        print(f\"Average Reward: {stats_random['avg_reward']:.2f}\")\n",
    "        print(\"\\nRandom Policy Visualization:\")\n",
    "        display_frames_as_gif(frames_random)\n",
    "        \n",
    "        # Trained policy trajectory\n",
    "        try:\n",
    "            print(\"\\nTrained Policy Trajectory:\")\n",
    "            config = MODEL_CONFIGS[env_id]\n",
    "            wrapped_model = load_and_wrap_model(config)\n",
    "            \n",
    "            frames_trained, stats_trained = collect_trajectory(env_id, policy=wrapped_model)\n",
    "            print(\"Trained Policy Stats:\")\n",
    "            print(f\"Episode Length: {stats_trained['length']}\")\n",
    "            print(f\"Total Reward: {stats_trained['total_reward']:.2f}\")\n",
    "            print(f\"Average Reward: {stats_trained['avg_reward']:.2f}\")\n",
    "            print(\"\\nTrained Policy Visualization:\")\n",
    "            display_frames_as_gif(frames_trained)\n",
    "            \n",
    "            improvement = ((stats_trained['total_reward'] - stats_random['total_reward']) \n",
    "                         / abs(stats_random['total_reward'])) * 100\n",
    "            print(f\"\\nImprovement in Total Reward: {improvement:.1f}%\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating trained model: {e}\")\n",
    "            raise  # Show full error traceback\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIGS = {\n",
    "    \"CartPole-v1\": {\n",
    "        \"repo_id\": \"sb3/demo-hf-CartPole-v1\",\n",
    "        \"filename\": \"ppo-CartPole-v1.zip\",\n",
    "        \"algorithm\": PPO\n",
    "    },\n",
    "    \"MountainCar-v0\": {\n",
    "        \"repo_id\": \"sb3/dqn-MountainCar-v0\",\n",
    "        \"filename\": \"dqn-MountainCar-v0.zip\",\n",
    "        \"algorithm\": DQN\n",
    "    },\n",
    "    \"Acrobot-v1\": {\n",
    "        \"repo_id\": \"sb3/dqn-Acrobot-v1\",\n",
    "        \"filename\": \"dqn-Acrobot-v1.zip\",\n",
    "        \"algorithm\": DQN\n",
    "    },\n",
    "    \"Pendulum-v1\": {\n",
    "        \"repo_id\": \"sb3/ddpg-Pendulum-v1\",\n",
    "        \"filename\": \"ddpg-Pendulum-v1.zip\",\n",
    "        \"algorithm\": DDPG\n",
    "    },\n",
    "    \"LunarLander-v2\": {\n",
    "        \"repo_id\": \"sb3/ppo-LunarLander-v2\",\n",
    "        \"filename\": \"ppo-LunarLander-v2.zip\",\n",
    "        \"algorithm\": PPO\n",
    "    },\n",
    "    \"BipedalWalker-v3\": {\n",
    "        \"repo_id\": \"sb3/ddpg-BipedalWalker-v3\",\n",
    "        \"filename\": \"ddpg-BipedalWalker-v3.zip\",\n",
    "        \"algorithm\": DDPG\n",
    "    },\n",
    "    \"Walker2d-v4\": {\n",
    "        \"repo_id\": \"jren123/sac-walker2d-v4\",\n",
    "        \"filename\": \"SAC-Walker2d-v4.zip\",\n",
    "        \"algorithm\": SAC\n",
    "    },\n",
    "    \"Humanoid-v4\": {\n",
    "        \"repo_id\": \"jren123/sac-humanoid-v4\", # https://huggingface.co/jren123/sac-humanoid-v4\n",
    "        \"filename\": \"SAC-Humanoid-v4.zip\",\n",
    "        \"algorithm\": SAC\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Run evaluation\n",
    "control_env_ids = [\n",
    "    \"CartPole-v1\",\n",
    "    \"MountainCar-v0\",\n",
    "    \"Acrobot-v1\",\n",
    "    \"Pendulum-v1\"\n",
    "]\n",
    "\n",
    "compare_policies(control_env_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box2D environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box2d_env_ids = [\n",
    "    \"LunarLander-v2\",     # Land a spacecraft\n",
    "    \"BipedalWalker-v3\"   # Make a 2D robot walk\n",
    "]\n",
    "\n",
    "env_ids, category_name = box2d_env_ids, \"box2d\"\n",
    "\n",
    "analyze_environment_category(env_ids, category_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pretrained Policies for Box2D environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "compare_policies(box2d_env_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atari Games\n",
    "\n",
    "Loading pretrained models for Atari games is a bit complicated because of the pre-processing of the states. So we will not do that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ale_py\n",
    "\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "atari_env_ids = [\n",
    "    \"ALE/Pong-v5\",        # Atari Pong\n",
    "    \"ALE/Breakout-v5\"     # Atari Breakout\n",
    "]\n",
    "\n",
    "env_ids, category_name = atari_env_ids, \"Atari\"\n",
    "analyze_environment_category(env_ids, category_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mujoco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mujoco_env_ids = [\n",
    "    \"Walker2d-v4\",          # Make a 2D one-legged robot hop\n",
    "    \"Humanoid-v4\"         # Make a 3D humanoid walk\n",
    "]\n",
    "\n",
    "env_ids, category_name = mujoco_env_ids, \"Mujoco\"\n",
    "analyze_environment_category(env_ids, category_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_policies(mujoco_env_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
